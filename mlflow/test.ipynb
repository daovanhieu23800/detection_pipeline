{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515f71cc90ce4c3db62e61b523a3ed58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1328: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw predictions: tensor([[[2.9184e+00, 2.4811e+01, 2.7877e+01,  ..., 4.9787e+02,\n",
      "          5.7356e+02, 5.7950e+02],\n",
      "         [3.3195e+00, 2.6033e+00, 2.4984e+00,  ..., 5.6190e+02,\n",
      "          5.6086e+02, 5.6282e+02],\n",
      "         [5.6160e+00, 4.5851e+01, 5.1095e+01,  ..., 2.8147e+02,\n",
      "          1.2626e+02, 1.2822e+02],\n",
      "         ...,\n",
      "         [1.6302e-07, 2.9225e-07, 4.4020e-07,  ..., 1.0410e-06,\n",
      "          1.2776e-06, 2.2958e-06],\n",
      "         [1.1281e-07, 1.7772e-07, 1.9181e-07,  ..., 1.2685e-06,\n",
      "          1.3668e-06, 1.5561e-06],\n",
      "         [4.1009e-07, 4.6186e-07, 4.9135e-07,  ..., 1.4077e-06,\n",
      "          1.3306e-06, 1.3260e-06]]])\n",
      "torch.Size([1, 84, 8400])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import mlflow.pytorch\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set the MLflow model URI.\n",
    "# If you used the MLflow Model Registry, you can load the model by its registered name and stage.\n",
    "# For example, loading the production model:\n",
    "MODEL_URI = \"models:/YOLOv8_TorchScript_Model/3\"\n",
    "\n",
    "# Alternatively, if you want to load from a specific run's artifact, uncomment the following line:\n",
    "# MODEL_URI = \"runs:/<run_id>/model_artifact\"\n",
    "\n",
    "# Define the expected input size for your model.\n",
    "# YOLOv8 models are often trained on 640x640 images, but adjust if needed.\n",
    "INPUT_SIZE = (640, 640)\n",
    "\n",
    "# Define the device for inference.\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# --- Preprocessing ---\n",
    "# Create a preprocessing pipeline. This converts the PIL image to a tensor,\n",
    "# resizes it to the required dimensions, and scales pixel values to [0, 1].\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(INPUT_SIZE),    # Resize image to match model input size\n",
    "    transforms.ToTensor(),            # Convert image to tensor and scale pixel values\n",
    "    # If your model requires normalization, add:\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "def load_model(model_uri: str) -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    Load a TorchScript model logged in MLflow.\n",
    "\n",
    "    Args:\n",
    "        model_uri (str): MLflow model URI.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: Loaded TorchScript model ready for inference.\n",
    "    \"\"\"\n",
    "    # Load the model using MLflow's PyTorch API\n",
    "    model = mlflow.pytorch.load_model(model_uri)\n",
    "      # Move the model to the appropriate device\n",
    "    model.eval()      # Set the model to evaluation mode\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_image(model: torch.nn.Module, image_path: str):\n",
    "    \"\"\"\n",
    "    Perform inference on an image using the loaded model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The loaded TorchScript YOLOv8 model.\n",
    "        image_path (str): Path to the input image.\n",
    "\n",
    "    Returns:\n",
    "        The raw output from the model. Postprocessing might be required depending on your export.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"Image file not found at: {image_path}\")\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    input_tensor = preprocess(image)\n",
    "    input_tensor = input_tensor.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Move input tensor to the correct device\n",
    "\n",
    "    # Perform inference without tracking gradients\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "\n",
    "    # Note: The output structure will depend on how the TorchScript export was performed.\n",
    "    # YOLOv8 exports might include integrated postprocessing (NMS, etc.) or might return raw predictions.\n",
    "    return outputs\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the path to the input image\n",
    "    image_path = \"../test_image/000000000328.jpg\"\n",
    "\n",
    "    # Load the model\n",
    "    model = load_model(MODEL_URI)\n",
    "\n",
    "    # Run prediction on the provided image\n",
    "    predictions = predict_image(model, image_path)\n",
    "\n",
    "    # Print or further process the predictions\n",
    "    print(\"Raw predictions:\", predictions)\n",
    "    print(predictions.shape)\n",
    "    # Optionally: Add postprocessing here if your export does not include NMS or threshold filtering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 84, 8400])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842e4e9c47e84479841abaae65f74b0e\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01de931a7bb0482b9dfe22920f68999d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact downloaded to: C:\\Users\\Admin\\AppData\\Local\\Temp\\tmpg2opy04u\\model_weights/yolov8n.pt\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration\n",
    "# ---------------------------\n",
    "mlflow.set_tracking_uri(\"http://localhost:8080\")\n",
    "target_run_name = \"YOLOv8_v1\"\n",
    "artifact_path = \"model_weights/yolov8n.pt\"  # The path to your artifact within the run\n",
    "\n",
    "# ---------------------------\n",
    "# Step 1: Search for the Run by Name\n",
    "# ---------------------------\n",
    "# Run names are stored as a tag \"mlflow.runName\". Filter runs based on that.\n",
    "run_id = mlflow.search_runs(search_all_experiments=True)['run_id'][0]\n",
    "print(run_id)\n",
    "# If multiple runs are found, select the most recent run using the 'start_time' column.\n",
    "\n",
    "downloaded_path = mlflow.artifacts.download_artifacts(\n",
    "    run_id=run_id,\n",
    "    artifact_path=artifact_path\n",
    ")\n",
    "\n",
    "print(f\"Artifact downloaded to: {downloaded_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:781: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(downloaded_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Users\\Admin\\Desktop\\try\\mlflow\\..\\test_image\\000000000328.jpg: 512x640 3 persons, 1 bench, 1 backpack, 2 ties, 1 book, 37.6ms\n",
      "Speed: 7.1ms preprocess, 37.6ms inference, 108.5ms postprocess per image at shape (1, 3, 512, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "result = model('../test_image/000000000328.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Boxes object with attributes:\n",
       "\n",
       "cls: tensor([ 0.,  0.,  0., 73., 27., 27., 24., 13.], device='cuda:0')\n",
       "conf: tensor([0.8925, 0.8672, 0.8569, 0.6029, 0.5812, 0.3420, 0.2933, 0.2565], device='cuda:0')\n",
       "data: tensor([[2.0172e+02, 9.0509e+01, 4.0114e+02, 4.7412e+02, 8.9248e-01, 0.0000e+00],\n",
       "        [4.5097e+01, 6.6411e+01, 2.6366e+02, 4.0951e+02, 8.6716e-01, 0.0000e+00],\n",
       "        [3.3829e+02, 6.8681e+01, 5.8670e+02, 4.8304e+02, 8.5687e-01, 0.0000e+00],\n",
       "        [2.4880e+02, 2.2124e+02, 3.3245e+02, 2.6056e+02, 6.0289e-01, 7.3000e+01],\n",
       "        [3.2016e+02, 1.7012e+02, 3.3753e+02, 2.3333e+02, 5.8116e-01, 2.7000e+01],\n",
       "        [4.6084e+02, 1.7738e+02, 4.7902e+02, 2.2015e+02, 3.4198e-01, 2.7000e+01],\n",
       "        [1.6073e+01, 1.5715e+02, 1.3660e+02, 2.4961e+02, 2.9330e-01, 2.4000e+01],\n",
       "        [2.0226e+02, 1.5518e+02, 5.8856e+02, 4.7754e+02, 2.5653e-01, 1.3000e+01]], device='cuda:0')\n",
       "id: None\n",
       "is_track: False\n",
       "orig_shape: (491, 640)\n",
       "shape: torch.Size([8, 6])\n",
       "xywh: tensor([[301.4291, 282.3124, 199.4194, 383.6064],\n",
       "        [154.3775, 237.9631, 218.5608, 343.1037],\n",
       "        [462.4934, 275.8593, 248.4041, 414.3575],\n",
       "        [290.6270, 240.9034,  83.6531,  39.3217],\n",
       "        [328.8456, 201.7244,  17.3788,  63.2173],\n",
       "        [469.9298, 198.7614,  18.1797,  42.7697],\n",
       "        [ 76.3363, 203.3790, 120.5264,  92.4551],\n",
       "        [395.4087, 316.3607, 386.2957, 322.3560]], device='cuda:0')\n",
       "xywhn: tensor([[0.4710, 0.5750, 0.3116, 0.7813],\n",
       "        [0.2412, 0.4846, 0.3415, 0.6988],\n",
       "        [0.7226, 0.5618, 0.3881, 0.8439],\n",
       "        [0.4541, 0.4906, 0.1307, 0.0801],\n",
       "        [0.5138, 0.4108, 0.0272, 0.1288],\n",
       "        [0.7343, 0.4048, 0.0284, 0.0871],\n",
       "        [0.1193, 0.4142, 0.1883, 0.1883],\n",
       "        [0.6178, 0.6443, 0.6036, 0.6565]], device='cuda:0')\n",
       "xyxy: tensor([[201.7193,  90.5092, 401.1388, 474.1156],\n",
       "        [ 45.0971,  66.4112, 263.6579, 409.5149],\n",
       "        [338.2914,  68.6805, 586.6955, 483.0381],\n",
       "        [248.8004, 221.2425, 332.4535, 260.5642],\n",
       "        [320.1562, 170.1158, 337.5350, 233.3330],\n",
       "        [460.8400, 177.3765, 479.0197, 220.1462],\n",
       "        [ 16.0731, 157.1515, 136.5995, 249.6066],\n",
       "        [202.2608, 155.1827, 588.5565, 477.5386]], device='cuda:0')\n",
       "xyxyn: tensor([[0.3152, 0.1843, 0.6268, 0.9656],\n",
       "        [0.0705, 0.1353, 0.4120, 0.8340],\n",
       "        [0.5286, 0.1399, 0.9167, 0.9838],\n",
       "        [0.3888, 0.4506, 0.5195, 0.5307],\n",
       "        [0.5002, 0.3465, 0.5274, 0.4752],\n",
       "        [0.7201, 0.3613, 0.7485, 0.4484],\n",
       "        [0.0251, 0.3201, 0.2134, 0.5084],\n",
       "        [0.3160, 0.3161, 0.9196, 0.9726]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].boxes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
